{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this area we will see how to split the data after they are seperated into features 'X' and Dependent variable vectors 'y'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a good model, all the data available is not consumed in the build. Always data is split into training and test sets.\n",
    "The model is built on the training sets and the predictions are always tested on the test sets.\n",
    "Test set is also used to evaluate a particular model and calculate the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally data is divided in to 80% training data and 20% test data. The percentage can also vary depending on the situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is divided into X_train, X_test and y_train, y_test\n",
    "Predictions are always done on y_test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets import all the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new Library is being introduced here. Sikit-learn. Scikit Learn is a robust library which brings ML into production system.\n",
    "It provides a range of supervised and unsupervised learning algorithms via a consistent interface in Python.\n",
    "Sikit-learn uses numpy extensively for high-performance linear algebra and array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Data_Ads.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the dataset using 'pd.read_csv' function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = dataset.iloc[:, [2, 3]].values : implies all rows of columns with index 2 and 3 are passed as features\n",
    "\n",
    "This is another way of slicing the datasets.\n",
    "We can also slice the columns and rows by passing the index numbers seperated by ',' comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, [2, 3]].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = dataset.iloc[:, 4].values implies all the rows and the column index 4 which is the last in our case,  will be passed to y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    19  19000]\n",
      " [    35  20000]\n",
      " [    26  43000]\n",
      " [    27  57000]\n",
      " [    19  76000]\n",
      " [    27  58000]\n",
      " [    27  84000]\n",
      " [    32 150000]\n",
      " [    25  33000]\n",
      " [    35  65000]\n",
      " [    26  80000]\n",
      " [    26  52000]\n",
      " [    20  86000]\n",
      " [    32  18000]\n",
      " [    18  82000]\n",
      " [    29  80000]\n",
      " [    47  25000]\n",
      " [    45  26000]\n",
      " [    46  28000]\n",
      " [    48  29000]\n",
      " [    45  22000]\n",
      " [    47  49000]\n",
      " [    48  41000]\n",
      " [    45  22000]\n",
      " [    46  23000]\n",
      " [    47  20000]\n",
      " [    49  28000]\n",
      " [    47  30000]\n",
      " [    29  43000]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have our dataset ready, we will split the data into training set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function used to split is 'train_test_split' which is imported from the Sikit-learn 'Sklearn.model_selection'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train is used to build and Test is used to evaluate the model or predict\n",
    "train_test_split is a method to Split arrays or matrices into random train and test subsets\n",
    "\n",
    "Call the 'train_test_split' function and pass our dataset X,y along with split parameters.\n",
    "    test_size: Represents the proportion of split. 0.25 means 25% split\n",
    "    random_state: Controls the shuffling applied to the data before applying the split.\n",
    "    train_size: Represent the proportion of the dataset to include in the train split\n",
    "    shuffle: Whether or not to shuffel the data\n",
    "    stratify: If not none, data is split in stratified fashion.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    27  58000]\n",
      " [    18  82000]\n",
      " [    25  33000]\n",
      " [    47  25000]\n",
      " [    48  41000]\n",
      " [    45  26000]\n",
      " [    35  20000]\n",
      " [    26  80000]\n",
      " [    29  43000]\n",
      " [    27  84000]\n",
      " [    19  76000]\n",
      " [    46  28000]\n",
      " [    48  29000]\n",
      " [    35  65000]\n",
      " [    32 150000]\n",
      " [    46  23000]\n",
      " [    27  57000]\n",
      " [    19  19000]\n",
      " [    47  49000]\n",
      " [    29  80000]\n",
      " [    20  86000]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   26 43000]\n",
      " [   45 22000]\n",
      " [   45 22000]\n",
      " [   32 18000]\n",
      " [   26 52000]\n",
      " [   47 20000]\n",
      " [   47 30000]\n",
      " [   49 28000]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the test split is 0.25, 25% of the data is split for testing and 75% for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
